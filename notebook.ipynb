{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "This tutorial is based on the official tutorial titled [Working With Data](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) by the `scikit-learn` team and a similarly-minded [tutorial](https://github.com/jonhare/WAIS-ML101) conducted by Dr Jonathon Hare in 2016. Compared to those versions, the current tutorial features a dataset that has been published by [Kaggle](https://www.kaggle.com) as part of a competition that they organised for [Detecting Insults in Social Commentary](https://www.kaggle.com/c/detecting-insults-in-social-commentary). Furthermore, a number of changes have been carried in the code of the above-mentioned tutorials in order to include `pandas` in the data pre-processing stage, and to assure compatibility with the updated version of `scikit-learn`.\n",
    "\n",
    "In this tutorial you will learn how to:\n",
    "* Extract feature vectors from text documents\n",
    "* Load, inspect and pre-process a dataset of comments on social media\n",
    "* Train a classifier to predict whether a comment on social media is insulting or not\n",
    "* Use Grid Search in order to tune better the hyper-parameters of your Machine Learning pipeline\n",
    "\n",
    "In order to run this iPython Notebook (Python 2), [Jupyter](http://jupyter.org/) should be installed in your machine. Besides Jupyter, the following Python packages should also be installed: (i) `pandas` and (ii) `scikit-learn`. The easiest way to install all of these together is with [Anaconda](https://www.anaconda.com/) (Windows, macOS and Linux installers available)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Kaggle Dataset\n",
    "For the purposes of this tutorial, we will be using a dataset of comments on social media along with their classification labels (i.e. \"insult\" or \"neutral\"). The dataset is encoded in binary-encoded `pickle` files which reside in `./Kaggle/train.p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_dataset_location = '../Kaggle/train.p' # The location of the Kaggle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "\n",
    "# Loading the binary-encoded pickle files from the designated location.\n",
    "with open(kaggle_dataset_location, 'rb') as f:\n",
    "    kaggle_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `kaggle_dataset` variable contains the dataset as a pythonic dictionary of lists. We will be using the `pandas` library in order to tranform this structure into a `pandas.DataFrame` which will simplify the data inspection and pre-processing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3947\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "kaggle_dataset_df = pd.DataFrame(kaggle_dataset)\n",
    "print len(kaggle_dataset_df) # Number of rows in the loaded DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the first 10 rows of the `DataFrame` in order to get an understanding of the structure of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Class                                            Comment             Date\n",
      "0   insult                                 You fuck your dad.  20120618192155Z\n",
      "1  neutral  i really don't understand your point.\\xa0 It s...  20120528192215Z\n",
      "2  neutral  A\\\\xc2\\\\xa0majority of Canadians can and has b...              NaN\n",
      "3  neutral  listen if you dont wanna get married to a man ...              NaN\n",
      "4  neutral  C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1eddn...  20120619094753Z\n",
      "5  neutral  @SDL OK, but I would hope they'd sign him to a...  20120620171226Z\n",
      "6  neutral                        Yeah and where are you now?  20120503012628Z\n",
      "7   insult  shut the fuck up. you and the rest of your fag...              NaN\n",
      "8   insult  Either you are fake or extremely stupid...mayb...  20120502173553Z\n",
      "9   insult  That you are an idiot who understands neither ...  20120620160512Z\n"
     ]
    }
   ],
   "source": [
    "print kaggle_dataset_df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2898"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(kaggle_dataset_df['Class'] == 'neutral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i really don't understand your point.\\xa0 It seems that you are mixing apples and oranges.\n",
      "@jdstorm dont wish him injury but it happened on its OWN and i DOUBT he's injured, he looked embarrassed to me\n"
     ]
    }
   ],
   "source": [
    "print kaggle_dataset_df['Comment'][1]\n",
    "print kaggle_dataset_df['Comment'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'Class', u'Comment', u'Date'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Print all the available columns of the dataset.\n",
    "print(kaggle_dataset_df.columns)\n",
    "# Define the target column which we want to predict.\n",
    "target_column = u'Class'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the category codes of each of the classes in the target-column.\n",
    "inputs = kaggle_dataset_df['Comment']\n",
    "outputs = kaggle_dataset_df['Class'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'insult', u'neutral']\n"
     ]
    }
   ],
   "source": [
    "class_names = kaggle_dataset_df['Class'].astype('category').cat.categories.tolist()\n",
    "print class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test the performance of our algorithm, it is very important to to test its performance on data that it hasn't *seen* during training. Luckily, `sklearn` includes an appropriate function that splits the items for a dataset into random train and test subsets.\n",
    "\n",
    "We set the portion of the original dataset that will be used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = 0.2\n",
    "# Set a random_state number for replicability of the experiments.\n",
    "random_state = 10\n",
    "# Split dataset into training and testing according to the test_size variable.\n",
    "# output_train and output_test are lists containing the classes' indices.\n",
    "input_train, input_test = train_test_split(inputs.tolist(), test_size=test_size, random_state=random_state)\n",
    "y_train, y_test = train_test_split(outputs.tolist(), test_size=test_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Features from Text: The Bag-of-Words Approach\n",
    "In order to be able to use text documents$^1$ as either input to Machine Learning algorithms, we need to follow a process that would turn them into numerical feature vectors. We generally refer to this process as *vectorisation*. The most intuitive way to do so is the **bags-of-words** approach, which is carried out as follows:\n",
    "1. Identify all the words that occur in the documents of a training set.\n",
    "2. Assign a fixed integer ID to each one of those words. For example in Python you could build a dictionary that would map each word to each corresponding integer ID:\n",
    " ```python\n",
    " dictionary = {'I': 1,\n",
    "               'study': 2,\n",
    "               'machine': 3,\n",
    "               'learning': 4,\n",
    "               ...}\n",
    " ```\n",
    "3. For each document in the training set, we count the number of occurrences of each word, and we store it in $X[d, w]$, as the value of the $w$-th feature for the $d$-th document, where $w$ is the index of the word in the dictionary.\n",
    "\n",
    "The bags-of-words representation implies that total number of features is the number of distinct words in the corpus, which typically is larger than 100k. \n",
    "\n",
    "While storing all these values in a `numpy` array would require substantial amount of memory, most values in $X$ will be zeros since for a given document only a small subset of the set of the distinct words in the dataset will be present. For this reason, we say that bags-of-words are typically high-dimensional sparse datasets. We can save a lot of memory by only storing the non-zero parts of the feature vectors in memory. `scipy.sparse` matrices are data structures that do exactly this, and scikit-learn has built-in support for these structures.\n",
    "\n",
    "In `scipy` text preprocessing, tokenising and stop-words (e.g. \"and\", \"or\" and \"that\") filtering are included in a high-level component that is able to build a dictionary of features and transform documents to feature vectors:\n",
    "\n",
    "$^1$ Text documents can vary substantially in length and writing style. In our case, we refer to text documents as the short-lengthed comments of our Kaggle dataset, but the techniques presented in this tutorial could work on much longer collections, such as articles or books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer instance at 0x7f5e7a9bc0e0>>,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "word_tokenizer = TweetTokenizer(preserve_case=False, \n",
    "                                strip_handles=True, \n",
    "                                reduce_len=True).tokenize\n",
    "\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 1),\n",
    "                                   stop_words=None,\n",
    "                                   tokenizer=word_tokenizer,\n",
    "                                   # Ignore terms that have a document frequency strictly higher than the given threshold.\n",
    "                                   # If float, the parameter represents a proportion of documents, integer absolute counts.\n",
    "                                   max_df=1.0,\n",
    "                                   # Ignore terms that have a document frequency strictly lower than the given threshold.\n",
    "                                   # If float, the parameter represents a proportion of documents, integer absolute counts.\n",
    "                                   min_df=1)\n",
    "count_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working on a Toy Example\n",
    "Let’s use it to tokenize and count the word occurrences of a minimalistic corpus of text documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_corpus = [u'You have studied Machine Learning',\n",
    "              u'I love Machine Learning',\n",
    "              u'Looking forward to #WAISAwayDay',\n",
    "              u'Have you studied Machine Learning']\n",
    "\n",
    "# Fits and tranforms the corpus in its bag-of-words representation.\n",
    "toy_count = count_vectorizer.fit_transform(toy_corpus) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the fitting process each team is assigned a unique integer index corresponding to a column in the resulting `toy_count` matrix (i.e. equivalent to the $X$ matrix that has been mentioned in the description of this section of the tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 10)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 7)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 9)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 5)\t1\n",
      "  (3, 4)\t1\n",
      "  (3, 7)\t1\n",
      "  (3, 8)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 10)\t1\n",
      "[[0 0 1 0 1 0 0 1 1 0 1]\n",
      " [0 0 0 1 1 0 1 1 0 0 0]\n",
      " [1 1 0 0 0 1 0 0 0 1 0]\n",
      " [0 0 1 0 1 0 0 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print toy_count # This is the memory-efficient representation of a sparse matrix.\n",
    "print toy_count.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the first and the last rows of the array are identical. This is happening because they correspond to comments with the same words, and, thus, are encoded in equal vectors, which leads to loss of valuable information. `CountVectorizer` also supports counts of n-grams of words or consecutive characters. N-grams are runs of consecutive characters or words, so for example in the case of word bi-grams, every consecutive pair of words would be a feature. Support for n-grams can be enabled by adjusting the `ngram_range` variable during the initialisation of the `CountVectorizer`.\n",
    "\n",
    "In the initialisation of `CountVectorizer` set the `ngram_range` variable to `(1, 2)`, and check the resulting `toy_count` matrix by running `toy_count.toarray()`. Do the results make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation of the columns can be retrieved as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'#waisawayday',\n",
       " u'forward',\n",
       " u'have',\n",
       " u'i',\n",
       " u'learning',\n",
       " u'looking',\n",
       " u'love',\n",
       " u'machine',\n",
       " u'studied',\n",
       " u'to',\n",
       " u'you']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the vectoriser is fitted, you can retrieve the index of a particular word in the dictionary by simply calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.vocabulary_.get(u'machine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further details about the functionality of `CountVectorizer` please refer [here](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working on the Kaggle Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"Only if Ronald McDonald gives him time off from his regular job as a McSlave at McDoonald's.\",\n",
       " u\"You're a fucking joke.\",\n",
       " u'Magic Number, Magic Underpants = No Big Deal',\n",
       " u\"YOU HAVE A BEAUTIFUL BODY. What's wrong with a man looking at a woman with a beautiful body ESPECIALLY NICE HIPS? If I'm stalker for that then every dude I know is a stalker as well.\",\n",
       " u'So, now we know what you are by your own identification, fool, will you kindly moveon.org and stop cluttering up the place and so we intelligent posters can post?']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print a small part of the comments that will be used for training.\n",
    "input_train[:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer instance at 0x7f5eb8953cb0>>,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "word_tokenizer = TweetTokenizer(preserve_case=False, \n",
    "                                strip_handles=True, \n",
    "                                reduce_len=True).tokenize\n",
    "\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 1),\n",
    "                                   stop_words=None,\n",
    "                                   tokenizer=word_tokenizer,\n",
    "                                   # Ignore terms that have a document frequency strictly higher than the given threshold.\n",
    "                                   # If float, the parameter represents a proportion of documents, integer absolute counts.\n",
    "                                   max_df=1.0,\n",
    "                                   # Ignore terms that have a document frequency strictly lower than the given threshold.\n",
    "                                   # If float, the parameter represents a proportion of documents, integer absolute counts.\n",
    "                                   min_df=1)\n",
    "count_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fits and tranforms the corpus in its bag-of-words representation.\n",
    "X_train_count = count_vectorizer.fit_transform(input_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the fitting process each team is assigned a unique integer index corresponding to a column in the resulting `X_train_count` matrix (i.e. equivalent to the $X$ matrix that has been mentioned in the description of this section of the tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3157, 14730)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 4 0 ... 0 0 0]\n",
      " [0 2 0 ... 0 0 0]\n",
      " [0 4 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print X_train_count.shape\n",
    "print X_train_count.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Occurrences To Frequencies\n",
    "Occurrence count is a good start. However, longer documents will have higher average count values than shorter documents, even though they might talk about similar topics. To avoid these potential discrepancies, it suffices to divide the number of occurrences of each word in a document by the total number of words in the document. The number of times a term occurs in a document, divided by the number of terms in a document is called the **term frequency** (**tf**).\n",
    "\n",
    "Another refinement on top of term frequency is to downscale weights for words that occur in many documents in the corpus, and are therefore less informative than those that occur only in a smaller portion of the corpus. In order to achieve this we can weight terms on the basis of the **inverse document frequency** (**idf**). The *document frequency* is the number of documents a given word occurs in; the inverse document frequency is often defined as the total number of documents in the corpurs divided by the document frequency.\n",
    "\n",
    "Combining tf and idf results in a *family of weightings* (tf is usually multiplied by idf, but there a few different variations of how idf is computed) known as **term frequency-inverse document frequency** (**tf–idf**).\n",
    "\n",
    "Both tf and tf–idf on our `toy_corpus` can be computed using `scikit-learn` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3386294361119891"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# idf = (np.log(4.0 / 2) + 1) * 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 11)\n",
      "[[0.   0.   0.2  0.   0.2  0.   0.   0.2  0.2  0.   0.2 ]\n",
      " [0.   0.   0.   0.25 0.25 0.   0.25 0.25 0.   0.   0.  ]\n",
      " [0.25 0.25 0.   0.   0.   0.25 0.   0.   0.   0.25 0.  ]\n",
      " [0.   0.   0.2  0.   0.2  0.   0.   0.2  0.2  0.   0.2 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# Computing tf using the counts that have been computed from the CountVectorizer.\n",
    "tf_transformer = TfidfTransformer(use_idf=False, norm='l1')\n",
    "X_train_tf = tf_transformer.fit_transform(toy_count)\n",
    "\n",
    "print X_train_tf.shape\n",
    "print X_train_tf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_count.toarray()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 11)\n",
      "[[0.         0.         0.22118748 0.         0.16821878 0.\n",
      "  0.         0.16821878 0.22118748 0.         0.22118748]\n",
      " [0.         0.         0.         0.32475635 0.17524365 0.\n",
      "  0.32475635 0.17524365 0.         0.         0.        ]\n",
      " [0.25       0.25       0.         0.         0.         0.25\n",
      "  0.         0.         0.         0.25       0.        ]\n",
      " [0.         0.         0.22118748 0.         0.16821878 0.\n",
      "  0.         0.16821878 0.22118748 0.         0.22118748]]\n"
     ]
    }
   ],
   "source": [
    "# Computing tf-idf using the counts that have been computed from the CountVectorizer.\n",
    "tfidf_transformer = TfidfTransformer(use_idf=True, norm='l1', smooth_idf=False)\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(toy_count)\n",
    "\n",
    "print X_train_tfidf.shape\n",
    "print X_train_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than transforming the raw counts with the `TfidfTransformer`, it is alternatively possible to use the `TfidfVectorizer` to directly parse the dataset. We compute tf-idf scores on the Kaggle dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3157, 1989)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer(ngram_range=(1, 2),\n",
    "                             stop_words='english',\n",
    "                             # tokenizer='word_tokenizer',\n",
    "                             # Ignore terms that have a document frequency strictly higher than the given threshold.\n",
    "                             # If float, the parameter represents a proportion of documents, integer absolute counts.\n",
    "                             max_df=0.75,\n",
    "                             # Ignore terms that have a document frequency strictly lower than the given threshold.\n",
    "                             # If float, the parameter represents a proportion of documents, integer absolute counts.\n",
    "                             min_df=5,\n",
    "                             # tf-idf parameters\n",
    "                             use_idf=True, norm='l1', smooth_idf=False)\n",
    "\n",
    "X_train_tfidf = tfidf_vect.fit_transform(input_train)\n",
    "\n",
    "print X_train_tfidf.shape\n",
    "print X_train_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try filtering out terms that are either too frequent or infrequent in the dataset by adjusting the `max_df` and `min_df` variable respectively. This is an easy way of not only filtering out the less informative words but also reducing the number of features (less storage complexity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Predictive Model using K-Nearest-Neighbours\n",
    "Now that we have our training features and the labels of each post, we can train a classifier to predict whether a message is insulting or not. Let's start with a KNN classifier, which provides a simple baseline, although is perhaps not the best classifier for this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=3).fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To try to predict the outcome on a new comment we need to extract the features using almost the same feature extracting chain as before. The differences are that we call (i) `transform` instead of `fit_transform` on the transformer or vectoriser, and (ii) `predict` on the classifier since they have both been fit to the training set.\n",
    "\n",
    "You can test your own comments by changing the text in the `test_comment` variable. Does your classifier identify all the insults properly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuck you!!: insult\n"
     ]
    }
   ],
   "source": [
    "test_comment = 'Fuck you!!'\n",
    "test_comment_tfidf = tfidf_vect.transform([test_comment])\n",
    "y_pred = knn_clf.predict(test_comment_tfidf)\n",
    "\n",
    "print'%s: %s' % (test_comment, class_names[y_pred[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Performance on the Test Set\n",
    "We will be evaluating the performance of our KNN classifier on the *unseen* data of the test set based on the accuracy metric. In a binary classification task, such as ours, the accuracy with which a model predicts a specific class $c$ (e.g. insults) is formally defined as:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\sum \\text{TP} + \\sum \\text{TN}}{\\sum \\text{TP} + \\sum \\text{FP} + \\sum \\text{TN} + \\sum \\text{FN}}\n",
    "\\end{align}\n",
    "where:\n",
    "* $\\text{TP}$ refers to True Positive predictions: both the predicted and the empirical labels are $c$\n",
    "* $\\text{TN}$ refers to True Negative predictions: both the predicted and the empirical labels are $\\neq c$\n",
    "* $\\text{FP}$ refers to False Positive predictions: the predicted label is $c$ but the empirical label $\\neq c$\n",
    "* $\\text{FN}$ refers to False Negative predictions: the predicted label is $\\neq c$ but the empirical label is $c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "X_test_tfidf = tfidf_vect.transform(input_test)\n",
    "y_pred = knn_clf.predict(X_test_tfidf)\n",
    "print 'Accuracy: %.2f' % (metrics.accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Pipeline\n",
    "In order to make our pipeline (i.e. vectoriser or transformer $\\rightarrow$ classifier) easier to work with, `scikit-learn` provides the `Pipeline` class that behaves like a compound classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "clf_pipeline = Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1, 1),\n",
    "                                               stop_words='english',\n",
    "                                               tokenizer=word_tokenizer,\n",
    "                                               # If float, the parameter represents a proportion of documents, integer absolute counts.\n",
    "                                               max_df=1.0,\n",
    "                                               # Ignore terms that have a document frequency strictly lower than the given threshold.\n",
    "                                               # If float, the parameter represents a proportion of documents, integer absolute counts.\n",
    "                                               min_df=1,\n",
    "                                               # tf-idf parameters\n",
    "                                               use_idf=True, norm='l1', smooth_idf=False)),\n",
    "                         ('clf', KNeighborsClassifier(n_neighbors=3))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The names `tfidf` and `clf` (classifier) are arbitrary. We shall see their use in the section on grid search, below. We can now train (on the training set) and test (on the test set) the model in a similar fashion to when we had all the different components separate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76\n"
     ]
    }
   ],
   "source": [
    "# Model Training\n",
    "clf_pipeline.fit(input_train, y_train)\n",
    "y_pred = clf_pipeline.predict(input_test) # We are making prediction on the test set.\n",
    "print 'Accuracy: %.2f' % (metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can do better with a linear Support Vector Machine (SVM). We can change the learner by just plugging a different classifier object into our pipeline as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=0.75, max_features=None, min_df=3,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=Fal...y='l2', power_t=0.5, random_state=10, shuffle=True,\n",
       "       tol=1e-05, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf_pipeline = Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1, 1),\n",
    "                                               stop_words='english',\n",
    "                                               tokenizer=word_tokenizer,\n",
    "                                               # If float, the parameter represents a proportion of documents, integer absolute counts.\n",
    "                                               max_df=0.75,\n",
    "                                               # Ignore terms that have a document frequency strictly lower than the given threshold.\n",
    "                                               # If float, the parameter represents a proportion of documents, integer absolute counts.\n",
    "                                               min_df=3,\n",
    "                                               # tf-idf parameters\n",
    "                                               use_idf=True, norm='l2', smooth_idf=False)),\n",
    "                         ('clf', SGDClassifier(loss='hinge',\n",
    "                                           penalty='l2',\n",
    "                                           tol=1e-5,\n",
    "                                           random_state=random_state))])\n",
    "# Model Training\n",
    "clf_pipeline.fit(input_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf_pipeline.predict(input_test) # We are making prediction on the test set.\n",
    "print 'Accuracy: %.2f' % (metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scikit-learn` further provides utilities for a more detailed performance analysis of the results using different metrics (i.e. precision, recall, and f1-score). Support refers to the number of samples that belong to each particular class.\n",
    "\n",
    "For further details about those you can have a look [here](https://en.wikipedia.org/wiki/Precision_and_recall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "     insult       0.66      0.64      0.65       186\n",
      "    neutral       0.89      0.90      0.89       604\n",
      "\n",
      "avg / total       0.84      0.84      0.84       790\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, \n",
    "                                    y_pred,\n",
    "                                    target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like we did before, we can test how well our classifier is doing by inputing our own comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuck you!: insult\n"
     ]
    }
   ],
   "source": [
    "test_comment = 'Fuck you!'\n",
    "y_pred = clf_pipeline.predict([test_comment])\n",
    "\n",
    "print'%s: %s' % (test_comment, class_names[y_pred[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Tuning using Grid Search\n",
    "We have already encountered some parameters such as `use_idf` in the `TfidfTransformer` (and `TfidfVectorizer`). Classifiers tend to have many parameters as well. For example `KNeighborsClassifier` includes parameter for the number of neighbours and `SGDClassifier` has a penalty parameter alpha and configurable loss and penalty terms in the objective function.\n",
    "\n",
    "Instead of tweaking the parameters of the various components of the chain, it is possible to run an exhaustive search of the best parameters on a grid of possible values. Let's use this to explore whether we can make the KNeighborsClassifier perform as well as our linear SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pipeline = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                         ('clf', KNeighborsClassifier(n_neighbors=3))])\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__n_neighbors': (1, 3, 5, 7, 9)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, such an exhaustive search can be expensive. If we have multiple CPU cores at our disposal, we can tell the grid searcher to try these eleven parameter combinations in parallel with the `n_jobs` parameter. If we give this parameter a value of -1, grid search will detect how many cores are installed and uses them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pipeline = GridSearchCV(knn_pipeline, param_grid=parameters, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid search instance behaves like a normal `scikit-learn` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=Tru...owski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
       "           weights='uniform'))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'tfidf__use_idf': (True, False), 'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)], 'clf__n_neighbors': (1, 3, 5, 7, 9)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_pipeline.fit(input_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the optimal parameters out by inspecting the object's `grid_scores_` attribute, which is a list of parameters/score pairs. To get the best scoring attributes, we do as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf__n_neighbors: 9\n",
      "tfidf__ngram_range: (1, 1)\n",
      "tfidf__use_idf: True\n",
      "0.805194805195\n"
     ]
    }
   ],
   "source": [
    "best_parameters, score, _ = max(knn_pipeline.grid_scores_, key=lambda x: x[1])\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "print score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test and see how well new comments are classified on our `knn_pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do not agree!: neutral\n"
     ]
    }
   ],
   "source": [
    "test_comment = 'I do not agree!'\n",
    "y_pred = knn_pipeline.predict([test_comment])\n",
    "\n",
    "print'%s: %s' % (test_comment, class_names[y_pred[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring K-Means Clustering\n",
    "Now that we have extracted features from our training documents we're in a position to experiment with clustering. We will use K-Means as its one of the most intuitive clustering methods, although it does have a few limitations.\n",
    "\n",
    "K-Means clustering with 5 clusters can be achieved as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=500, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "num_clusters = 500\n",
    "k_means = KMeans(num_clusters)\n",
    "k_means.fit(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assignments of the original posts to cluster id is given by `km.labels_` once `km.fit(...)` has been called. The centroids of the clusters is given by `km.cluster_centers_`. Intuitively, the vector that describes the centre of a cluster is just like any other feature vector. An interesting way to explore what each cluster is representing is to calculate and print the top weighted (either by occurrence or tf-idf) terms for that cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:  bitches\n",
      " money\n",
      " ball\n",
      " yo\n",
      " nigga\n",
      "Cluster 1:  posted\n",
      " control\n",
      " hope\n",
      " sure\n",
      " comment\n",
      "Cluster 2:  ho\n",
      " da\n",
      " xa0\n",
      " 2011\n",
      " fox news\n",
      "Cluster 3:  fuck\n",
      " zero\n",
      " fucked\n",
      " forum\n",
      " forward\n",
      "Cluster 4:  night\n",
      " tells\n",
      " laugh\n",
      " live\n",
      " sleep\n",
      "Cluster 5:  don\n",
      " zero\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 6:  washington\n",
      " zero\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 7:  lock\n",
      " hey\n",
      " idiot\n",
      " zero\n",
      " forum\n",
      "Cluster 8:  goes\n",
      " zero\n",
      " fuck just\n",
      " forum\n",
      " forward\n",
      "Cluster 9:  xc2\n",
      " xc2 xa0\n",
      " xa0\n",
      " blow\n",
      " gay\n",
      "Cluster 10:  came\n",
      " idiot\n",
      " zero\n",
      " fucker\n",
      " fox\n",
      "Cluster 11:  don\n",
      " like\n",
      " zero\n",
      " friend\n",
      " fuck just\n",
      "Cluster 12:  fool\n",
      " really\n",
      " fucker\n",
      " fox\n",
      " fox news\n",
      "Cluster 13:  moron\n",
      " zero\n",
      " fuckin\n",
      " fox\n",
      " fox news\n",
      "Cluster 14:  complete\n",
      " idiot\n",
      " sir\n",
      " waste\n",
      " president\n",
      "Cluster 15:  holy shit\n",
      " holy\n",
      " shit\n",
      " didn\n",
      " science\n",
      "Cluster 16:  idiot\n",
      " father\n",
      " death\n",
      " need\n",
      " fucking\n",
      "Cluster 17:  xa0\n",
      " zero\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 18:  wearing\n",
      " head\n",
      " forgot\n",
      " forum\n",
      " forward\n",
      "Cluster 19:  talking\n",
      " fucked\n",
      " forum\n",
      " forward\n",
      " fox\n",
      "Cluster 20:  sarcasm\n",
      " forgot\n",
      " forum\n",
      " forward\n",
      " fox\n",
      "Cluster 21:  conservative\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 22:  azz\n",
      " zero\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 23:  figure\n",
      " stupid\n",
      " word\n",
      " related\n",
      " defend\n",
      "Cluster 24:  correct\n",
      " zero\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 25:  country\n",
      " did\n",
      " forward\n",
      " fox\n",
      " fox news\n",
      "Cluster 26:  haha\n",
      " like\n",
      " free\n",
      " fuck\n",
      " friggin\n",
      "Cluster 27:  offensive\n",
      " stupid\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 28:  think\n",
      " zero\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 29:  http\n",
      " zero\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 30:  disgusting\n",
      " fucked\n",
      " forum\n",
      " forward\n",
      " fox\n",
      "Cluster 31:  sucks\n",
      " zero\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 32:  oh\n",
      " stupid\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 33:  shit\n",
      " zero\n",
      " fucked\n",
      " forum\n",
      " forward\n",
      "Cluster 34:  dumb\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      " fox news\n",
      "Cluster 35:  right\n",
      " zero\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 36:  yes\n",
      " wrong\n",
      " time\n",
      " moron\n",
      " just\n",
      "Cluster 37:  away\n",
      " little\n",
      " zero\n",
      " freedom\n",
      " fuck just\n",
      "Cluster 38:  wrong\n",
      " zero\n",
      " fucked\n",
      " forum\n",
      " forward\n",
      "Cluster 39:  horse\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      " fox news\n",
      "Cluster 40:  fan\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 41:  faggot\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 42:  clown\n",
      " zero\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 43:  xa0\n",
      " xa0 xa0\n",
      " xa0 just\n",
      " just\n",
      " like\n",
      "Cluster 44:  racist\n",
      " disagree\n",
      " bitch\n",
      " zero\n",
      " fucker\n",
      "Cluster 45:  like\n",
      " just\n",
      " don\n",
      " know\n",
      " people\n",
      "Cluster 46:  victim\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 47:  rock\n",
      " living\n",
      " zero\n",
      " freedom\n",
      " fuck just\n",
      "Cluster 48:  liked\n",
      " comment\n",
      " far\n",
      " zero\n",
      " fucked\n",
      "Cluster 49:  nice\n",
      " shit\n",
      " zero\n",
      " fuck just\n",
      " forum\n",
      "Cluster 50:  maggot\n",
      " change\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 51:  retard\n",
      " went\n",
      " just\n",
      " zero\n",
      " fuck\n",
      "Cluster 52:  pathetic\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 53:  fucking\n",
      " zero\n",
      " fucked\n",
      " forum\n",
      " forward\n",
      "Cluster 54:  cool\n",
      " actually\n",
      " think\n",
      " zero\n",
      " friend\n",
      "Cluster 55:  sure\n",
      " zero\n",
      " forward\n",
      " fox\n",
      " fox news\n",
      "Cluster 56:  reply\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 57:  loves\n",
      " obama\n",
      " know\n",
      " forum\n",
      " fox\n",
      "Cluster 58:  old\n",
      " zero\n",
      " fuckin\n",
      " fox\n",
      " fox news\n",
      "Cluster 59:  sir\n",
      " intelligent\n",
      " said\n",
      " great\n",
      " fuck\n",
      "Cluster 60:  game\n",
      " zero\n",
      " fucked\n",
      " forum\n",
      " forward\n",
      "Cluster 61:  sec\n",
      " fucked\n",
      " forum\n",
      " forward\n",
      " fox\n",
      "Cluster 62:  believe\n",
      " make\n",
      " fuckin\n",
      " fox\n",
      " fox news\n",
      "Cluster 63:  retarded\n",
      " clearly\n",
      " play\n",
      " fucking\n",
      " zero\n",
      "Cluster 64:  outside\n",
      " heat\n",
      " bad\n",
      " fuck\n",
      " friggin\n",
      "Cluster 65:  gotten\n",
      " good\n",
      " zero\n",
      " forgot\n",
      " forum\n",
      "Cluster 66:  balls\n",
      " zero\n",
      " fuckin\n",
      " fox\n",
      " fox news\n",
      "Cluster 67:  wet\n",
      " zero\n",
      " fucked\n",
      " forum\n",
      " forward\n",
      "Cluster 68:  things\n",
      " zero\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 69:  hate\n",
      " man\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 70:  welcome\n",
      " old\n",
      " zero\n",
      " fuck just\n",
      " fuck\n",
      "Cluster 71:  boy\n",
      " zero\n",
      " fucker\n",
      " fox\n",
      " fox news\n",
      "Cluster 72:  truth\n",
      " say\n",
      " zero\n",
      " forgot\n",
      " forum\n",
      "Cluster 73:  aren\n",
      " little\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 74:  ugly\n",
      " mother\n",
      " zero\n",
      " freedom\n",
      " fuck just\n",
      "Cluster 75:  hey\n",
      " good\n",
      " zero\n",
      " fucked\n",
      " fox\n",
      "Cluster 76:  http\n",
      " try\n",
      " bit\n",
      " definitely\n",
      " damn\n",
      "Cluster 77:  hope\n",
      " zero\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 78:  dollar\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      " fox news\n",
      "Cluster 79:  like\n",
      " zero\n",
      " fucked\n",
      " forum\n",
      " forward\n",
      "Cluster 80:  break\n",
      " zero\n",
      " fucker\n",
      " fox\n",
      " fox news\n",
      "Cluster 81:  turd\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 82:  loser\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      " fox news\n",
      "Cluster 83:  na\n",
      " zero\n",
      " fucker\n",
      " fox\n",
      " fox news\n",
      "Cluster 84:  style\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      " fox news\n",
      "Cluster 85:  idiot\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 86:  women\n",
      " zero\n",
      " fuck just\n",
      " forum\n",
      " forward\n",
      "Cluster 87:  say\n",
      " fuck\n",
      " don\n",
      " zero\n",
      " fucked\n",
      "Cluster 88:  joking\n",
      " yes\n",
      " neil\n",
      " idiot\n",
      " fuck just\n",
      "Cluster 89:  stupid\n",
      " zero\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 90:  shouldn\n",
      " zero\n",
      " fucked\n",
      " forum\n",
      " forward\n",
      "Cluster 91:  illegal\n",
      " fucked\n",
      " forum\n",
      " forward\n",
      " fox\n",
      "Cluster 92:  thread\n",
      " zero\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 93:  faggot\n",
      " gay\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 94:  mate\n",
      " zero\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 95:  easy\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 96:  idiots\n",
      " fucking\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 97:  brain\n",
      " tell\n",
      " lol\n",
      " zero\n",
      " fox\n",
      "Cluster 98:  sick\n",
      " fuck\n",
      " zero\n",
      " fuck just\n",
      " forum\n",
      "Cluster 99:  level\n",
      " zero\n",
      " forgot\n",
      " forum\n",
      " forward\n",
      "Cluster 100:  hole\n",
      " zero\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 101:  morons\n",
      " zero\n",
      " fuckin\n",
      " fox\n",
      " fox news\n",
      "Cluster 102:  fat\n",
      " like\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 103:  stupid\n",
      " really\n",
      " zero\n",
      " fuck just\n",
      " forum\n",
      "Cluster 104:  second\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 105:  point\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 106:  fan\n",
      " team\n",
      " zero\n",
      " form\n",
      " forward\n",
      "Cluster 107:  chicago\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 108:  favorite\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 109:  pig\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      " fox news\n",
      "Cluster 110:  jewish\n",
      " zero\n",
      " fuck just\n",
      " forum\n",
      " forward\n",
      "Cluster 111:  stupidity\n",
      " zero\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 112:  bless\n",
      " fuckin\n",
      " fox\n",
      " fox news\n",
      " fraud\n",
      "Cluster 113:  hater\n",
      " zero\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 114:  fail\n",
      " loser\n",
      " fucker\n",
      " fox\n",
      " fox news\n",
      "Cluster 115:  hit\n",
      " come\n",
      " zero\n",
      " fox\n",
      " fox news\n",
      "Cluster 116:  mind\n",
      " know\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 117:  opinion\n",
      " better\n",
      " good\n",
      " just\n",
      " like\n",
      "Cluster 118:  home\n",
      " zero\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 119:  existence\n",
      " zero\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 120:  david\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 121:  suck\n",
      " zero\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 122:  shove\n",
      " zero\n",
      " fucked\n",
      " forum\n",
      " forward\n",
      "Cluster 123:  nobjustryan\n",
      " thing\n",
      " freedom\n",
      " fuck just\n",
      " fuck\n",
      "Cluster 124:  dickhead\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      " fox news\n",
      "Cluster 125:  flat\n",
      " zero\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 126:  just\n",
      " zero\n",
      " fuck just\n",
      " forum\n",
      " forward\n",
      "Cluster 127:  yeah\n",
      " zero\n",
      " fucked\n",
      " forum\n",
      " forward\n",
      "Cluster 128:  dude\n",
      " funny\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 129:  apple\n",
      " fucking\n",
      " lol\n",
      " shit\n",
      " free\n",
      "Cluster 130:  possibly\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 131:  effective\n",
      " fucked\n",
      " forum\n",
      " forward\n",
      " fox\n",
      "Cluster 132:  taxpayers\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 133:  magic\n",
      " handle\n",
      " don\n",
      " big deal\n",
      " number\n",
      "Cluster 134:  50\n",
      " teams\n",
      " winning\n",
      " shot\n",
      " game\n",
      "Cluster 135:  islamic\n",
      " zero\n",
      " fucked\n",
      " forum\n",
      " forward\n",
      "Cluster 136:  xa0that\n",
      " zero\n",
      " forgot\n",
      " forum\n",
      " forward\n",
      "Cluster 137:  whore\n",
      " obama\n",
      " fuck\n",
      " like\n",
      " forward\n",
      "Cluster 138:  friend\n",
      " zero\n",
      " fucked\n",
      " forum\n",
      " forward\n",
      "Cluster 139:  dick\n",
      " better\n",
      " guy\n",
      " nyour\n",
      " idiot\n",
      "Cluster 140:  mad\n",
      " zero\n",
      " fuckin\n",
      " fox\n",
      " fox news\n",
      "Cluster 141:  white\n",
      " zero\n",
      " fuck just\n",
      " forum\n",
      " forward\n",
      "Cluster 142:  canada\n",
      " zero\n",
      " fuckin\n",
      " fox\n",
      " fox news\n",
      "Cluster 143:  baby\n",
      " zero\n",
      " fuckin\n",
      " fox\n",
      " fox news\n",
      "Cluster 144:  dad\n",
      " fuck\n",
      " fucked\n",
      " forum\n",
      " forward\n",
      "Cluster 145:  watching\n",
      " zero\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 146:  nto\n",
      " zero\n",
      " fucker\n",
      " fox\n",
      " fox news\n",
      "Cluster 147:  flag\n",
      " zero\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 148:  burn\n",
      " zero\n",
      " fucker\n",
      " fox\n",
      " fox news\n",
      "Cluster 149:  shirt\n",
      " got\n",
      " nice\n",
      " fuck just\n",
      " forum\n",
      "Cluster 150:  000\n",
      " fucked\n",
      " forum\n",
      " forward\n",
      " fox\n",
      "Cluster 151:  yep\n",
      " zero\n",
      " fucked\n",
      " forum\n",
      " forward\n",
      "Cluster 152:  okc\n",
      " zero\n",
      " fuckin\n",
      " fox\n",
      " fox news\n",
      "Cluster 153:  coach\n",
      " looks\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 154:  evidence\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      " fox news\n",
      "Cluster 155:  built\n",
      " like\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 156:  wife\n",
      " child\n",
      " zero\n",
      " form\n",
      " forward\n",
      "Cluster 157:  song\n",
      " make\n",
      " zero\n",
      " fuck\n",
      " friggin\n",
      "Cluster 158:  retards\n",
      " like\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 159:  shut fuck\n",
      " shut\n",
      " fuck\n",
      " eat\n",
      " blaming\n",
      "Cluster 160:  california\n",
      " people\n",
      " zero\n",
      " fucker\n",
      " fox\n",
      "Cluster 161:  hey\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 162:  england\n",
      " century\n",
      " new\n",
      " zero\n",
      " fox\n",
      "Cluster 163:  worthless\n",
      " person\n",
      " post\n",
      " zero\n",
      " forward\n",
      "Cluster 164:  bro\n",
      " left\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 165:  box\n",
      " zero\n",
      " fucker\n",
      " fox\n",
      " fox news\n",
      "Cluster 166:  sorry\n",
      " right\n",
      " fuck just\n",
      " form\n",
      " forum\n",
      "Cluster 167:  libturd\n",
      " zero\n",
      " forgot\n",
      " forum\n",
      " forward\n",
      "Cluster 168:  typical\n",
      " zero\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 169:  google\n",
      " fuck\n",
      " zero\n",
      " fuck just\n",
      " forum\n",
      "Cluster 170:  coming\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 171:  jerk\n",
      " way\n",
      " zero\n",
      " forum\n",
      " forward\n",
      "Cluster 172:  sound like\n",
      " sound\n",
      " like\n",
      " dickhead\n",
      " man\n",
      "Cluster 173:  sure\n",
      " man\n",
      " xa0\n",
      " freedom\n",
      " fuck just\n",
      "Cluster 174:  change\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 175:  welcome\n",
      " zero\n",
      " fucked\n",
      " forum\n",
      " forward\n",
      "Cluster 176:  brothers\n",
      " sounds like\n",
      " meet\n",
      " happened\n",
      " sounds\n",
      "Cluster 177:  daily\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 178:  fake\n",
      " face\n",
      " dick\n",
      " shit\n",
      " ass\n",
      "Cluster 179:  hell\n",
      " fuck\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 180:  land\n",
      " zero\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 181:  tonight\n",
      " playing\n",
      " make\n",
      " friend\n",
      " fucked\n",
      "Cluster 182:  post\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 183:  season\n",
      " start\n",
      " zero\n",
      " fuck just\n",
      " forum\n",
      "Cluster 184:  played\n",
      " time\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 185:  showing\n",
      " little\n",
      " zero\n",
      " fuck just\n",
      " forum\n",
      "Cluster 186:  born\n",
      " zero\n",
      " fuckin\n",
      " fox\n",
      " fox news\n",
      "Cluster 187:  let\n",
      " know\n",
      " say\n",
      " hate\n",
      " man\n",
      "Cluster 188:  miss\n",
      " hit\n",
      " like\n",
      " zero\n",
      " fuck just\n",
      "Cluster 189:  lies\n",
      " zero\n",
      " forgot\n",
      " forum\n",
      " forward\n",
      "Cluster 190:  el\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 191:  moronic\n",
      " people\n",
      " zero\n",
      " fucker\n",
      " fox\n",
      "Cluster 192:  looks like\n",
      " looks\n",
      " like\n",
      " fuck\n",
      " eat\n",
      "Cluster 193:  fucking\n",
      " fuck\n",
      " zero\n",
      " fucked\n",
      " forum\n",
      "Cluster 194:  air\n",
      " dont\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 195:  kidding\n",
      " zero\n",
      " fucked\n",
      " forum\n",
      " forward\n",
      "Cluster 196:  mom\n",
      " like\n",
      " zero\n",
      " freedom\n",
      " fuck just\n",
      "Cluster 197:  fake\n",
      " fucking\n",
      " fuck\n",
      " fucked\n",
      " forum\n",
      "Cluster 198:  question\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 199:  massive\n",
      " fucking\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 200:  durant\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 201:  words\n",
      " zero\n",
      " fuck just\n",
      " forum\n",
      " forward\n",
      "Cluster 202:  love\n",
      " fucking\n",
      " fuck\n",
      " zero\n",
      " fucker\n",
      "Cluster 203:  problem\n",
      " zero\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 204:  bigot\n",
      " like\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 205:  want\n",
      " fuck\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 206:  living\n",
      " zero\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 207:  heat\n",
      " people\n",
      " zero\n",
      " forum\n",
      " fox\n",
      "Cluster 208:  come\n",
      " zero\n",
      " forward\n",
      " fox\n",
      " fox news\n",
      "Cluster 209:  beautiful\n",
      " zero\n",
      " fucker\n",
      " fox\n",
      " fox news\n",
      "Cluster 210:  read\n",
      " did\n",
      " zero\n",
      " fox\n",
      " fox news\n",
      "Cluster 211:  cover\n",
      " issue\n",
      " xa0\n",
      " zero\n",
      " freedom\n",
      "Cluster 212:  anti\n",
      " racist\n",
      " moron\n",
      " zero\n",
      " fuck just\n",
      "Cluster 213:  ve\n",
      " know\n",
      " zero\n",
      " forward\n",
      " fox\n",
      "Cluster 214:  hard\n",
      " fuck\n",
      " zero\n",
      " fucker\n",
      " forward\n",
      "Cluster 215:  sites\n",
      " goes\n",
      " zero\n",
      " forgot\n",
      " forum\n",
      "Cluster 216:  whore\n",
      " mother\n",
      " xa0\n",
      " zero\n",
      " freedom\n",
      "Cluster 217:  yinzerinct\n",
      " fun\n",
      " zero\n",
      " free\n",
      " fuck\n",
      "Cluster 218:  headed\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 219:  dress\n",
      " zero\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 220:  bet\n",
      " yeah\n",
      " zero\n",
      " fucker\n",
      " fox\n",
      "Cluster 221:  starts\n",
      " story\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 222:  100\n",
      " right\n",
      " congress\n",
      " yes\n",
      " freak\n",
      "Cluster 223:  fuckin\n",
      " man\n",
      " gas\n",
      " fucked\n",
      " forward\n",
      "Cluster 224:  wanna\n",
      " know\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 225:  talented\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 226:  lol\n",
      " man\n",
      " form\n",
      " forward\n",
      " fox\n",
      "Cluster 227:  better\n",
      " zero\n",
      " fucker\n",
      " fox\n",
      " fox news\n",
      "Cluster 228:  run\n",
      " home\n",
      " children\n",
      " government\n",
      " zero\n",
      "Cluster 229:  didnt\n",
      " pay\n",
      " nbut\n",
      " mind\n",
      " like\n",
      "Cluster 230:  senator\n",
      " thing\n",
      " zero\n",
      " fuck just\n",
      " forum\n",
      "Cluster 231:  dog\n",
      " girl\n",
      " nigga\n",
      " fucked\n",
      " forward\n",
      "Cluster 232:  paul\n",
      " ron\n",
      " ron paul\n",
      " damn\n",
      " don forget\n",
      "Cluster 233:  presidents\n",
      " best\n",
      " zero\n",
      " fucker\n",
      " fox\n",
      "Cluster 234:  fair\n",
      " man\n",
      " fucker\n",
      " fox\n",
      " fox news\n",
      "Cluster 235:  logic\n",
      " zero\n",
      " form\n",
      " forward\n",
      " fox\n",
      "Cluster 236:  prefer\n",
      " work\n",
      " zero\n",
      " fuck\n",
      " forum\n",
      "Cluster 237:  xa0http\n",
      " com\n",
      " funny\n",
      " xa0\n",
      " free\n",
      "Cluster 238:  wasting\n",
      " time\n",
      " zero\n",
      " fuck just\n",
      " fuck\n",
      "Cluster 239:  manager\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      " fox news\n",
      "Cluster 240:  lots\n",
      " free\n",
      " zero\n",
      " fucker\n",
      " fox\n",
      "Cluster 241:  yesterday\n",
      " stupid\n",
      " zero\n",
      " free\n",
      " fuck\n",
      "Cluster 242:  looked like\n",
      " looked\n",
      " like\n",
      " picture\n",
      " end\n",
      "Cluster 243:  kick\n",
      " ass\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 244:  used\n",
      " like\n",
      " freedom\n",
      " fuck just\n",
      " fuck\n",
      "Cluster 245:  careful\n",
      " zero\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 246:  team\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 247:  hater\n",
      " simple\n",
      " zero\n",
      " fuck just\n",
      " forum\n",
      "Cluster 248:  insane\n",
      " zero\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 249:  ho\n",
      " hey\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " di\n",
      " ban\n",
      " la\n",
      "Cluster 250:  eat\n",
      " just\n",
      " zero\n",
      " form\n",
      " forward\n",
      "Cluster 251:  nget\n",
      " fuck\n",
      " zero\n",
      " fuckin\n",
      " fox\n",
      "Cluster 252:  really\n",
      " like\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 253:  just like\n",
      " just\n",
      " like\n",
      " media\n",
      " popularity\n",
      "Cluster 254:  ya\n",
      " know\n",
      " dumb\n",
      " fuck\n",
      " freedom\n",
      "Cluster 255:  exactly\n",
      " zero\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 256:  continue\n",
      " comes\n",
      " fucker\n",
      " fox\n",
      " fox news\n",
      "Cluster 257:  interesting\n",
      " think\n",
      " writing\n",
      " gullible\n",
      " enemies\n",
      "Cluster 258:  joke\n",
      " fucking\n",
      " zero\n",
      " fuck just\n",
      " forum\n",
      "Cluster 259:  obama\n",
      " xa0\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 260:  commenter\n",
      " trolling\n",
      " loser\n",
      " fucked\n",
      " forward\n",
      "Cluster 261:  scumbag\n",
      " carter\n",
      " read\n",
      " forum\n",
      " forward\n",
      "Cluster 262:  elected\n",
      " better\n",
      " zero\n",
      " fucker\n",
      " fox news\n",
      "Cluster 263:  credit\n",
      " like\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 264:  ego\n",
      " makes\n",
      " zero\n",
      " fucker\n",
      " fox\n",
      "Cluster 265:  important\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 266:  happening\n",
      " stay\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 267:  rock\n",
      " world\n",
      " zero\n",
      " forum\n",
      " forward\n",
      "Cluster 268:  useful\n",
      " nthe\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 269:  double\n",
      " retard\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 270:  group\n",
      " zero\n",
      " fuck just\n",
      " forum\n",
      " forward\n",
      "Cluster 271:  child\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 272:  oh\n",
      " zero\n",
      " fuckin\n",
      " fox\n",
      " fox news\n",
      "Cluster 273:  forget\n",
      " fuck\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 274:  pass\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 275:  dumb\n",
      " fucking\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 276:  make\n",
      " zero\n",
      " fuckin\n",
      " fox\n",
      " fox news\n",
      "Cluster 277:  wide\n",
      " zero\n",
      " fuck just\n",
      " forum\n",
      " forward\n",
      "Cluster 278:  night\n",
      " tell\n",
      " zero\n",
      " forward\n",
      " fox\n",
      "Cluster 279:  ipod\n",
      " touch\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 280:  god\n",
      " mouth\n",
      " fuck\n",
      " zero\n",
      " fucker\n",
      "Cluster 281:  gorgeous\n",
      " fucking\n",
      " fuck just\n",
      " form\n",
      " forum\n",
      "Cluster 282:  away\n",
      " just\n",
      " fucked\n",
      " forum\n",
      " forward\n",
      "Cluster 283:  girls\n",
      " fuck\n",
      " love\n",
      " just\n",
      " zero\n",
      "Cluster 284:  water\n",
      " zero\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 285:  thoughts\n",
      " family\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 286:  wow\n",
      " really\n",
      " zero\n",
      " fuck just\n",
      " forum\n",
      "Cluster 287:  guy\n",
      " real\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 288:  lol\n",
      " dont\n",
      " used\n",
      " money\n",
      " fox\n",
      "Cluster 289:  picking\n",
      " people\n",
      " zero\n",
      " fucker\n",
      " fox\n",
      "Cluster 290:  amazing\n",
      " wife\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 291:  sounds like\n",
      " sounds\n",
      " like\n",
      " medical\n",
      " family\n",
      "Cluster 292:  shit\n",
      " did\n",
      " zero\n",
      " fucker\n",
      " fox\n",
      "Cluster 293:  kidding\n",
      " fucking\n",
      " fuck\n",
      " zero\n",
      " fuck just\n",
      "Cluster 294:  welcome\n",
      " thanks\n",
      " free\n",
      " fuck\n",
      " friggin\n",
      "Cluster 295:  soooo\n",
      " dumb\n",
      " zero\n",
      " freedom\n",
      " fuck just\n",
      "Cluster 296:  queen\n",
      " like\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 297:  blind\n",
      " planet\n",
      " live\n",
      " stupid\n",
      " need\n",
      "Cluster 298:  drug\n",
      " fine\n",
      " loser\n",
      " good\n",
      " fuck just\n",
      "Cluster 299:  cunt\n",
      " people\n",
      " zero\n",
      " form\n",
      " forward\n",
      "Cluster 300:  leader\n",
      " party\n",
      " zero\n",
      " form\n",
      " forward\n",
      "Cluster 301:  cheap\n",
      " people\n",
      " zero\n",
      " forum\n",
      " fox\n",
      "Cluster 302:  mention\n",
      " come\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 303:  reply\n",
      " lol\n",
      " zero\n",
      " form\n",
      " forward\n",
      "Cluster 304:  te\n",
      " ya\n",
      " forgot\n",
      " forum\n",
      " forward\n",
      "Cluster 305:  profile\n",
      " look\n",
      " need\n",
      " zero\n",
      " fuck just\n",
      "Cluster 306:  join\n",
      " hollywood\n",
      " zero\n",
      " fuck just\n",
      " forum\n",
      "Cluster 307:  cnn\n",
      " boss\n",
      " fuckin\n",
      " fox\n",
      " fox news\n",
      "Cluster 308:  lie\n",
      " time\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 309:  type\n",
      " comment\n",
      " zero\n",
      " fucker\n",
      " fox\n",
      "Cluster 310:  term\n",
      " correct\n",
      " life\n",
      " freedom\n",
      " fuck just\n",
      "Cluster 311:  quickly\n",
      " killing\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 312:  road\n",
      " lol\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 313:  claim\n",
      " man\n",
      " fuckin\n",
      " fox\n",
      " fox news\n",
      "Cluster 314:  rest\n",
      " doesn\n",
      " zero\n",
      " form\n",
      " forward\n",
      "Cluster 315:  500\n",
      " better\n",
      " zero\n",
      " forum\n",
      " fox\n",
      "Cluster 316:  color\n",
      " blind\n",
      " fuckin\n",
      " fox\n",
      " fox news\n",
      "Cluster 317:  win\n",
      " david\n",
      " dont\n",
      " 10\n",
      " think\n",
      "Cluster 318:  huge\n",
      " guess\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 319:  lebron\n",
      " wins\n",
      " fan\n",
      " zero\n",
      " forum\n",
      "Cluster 320:  realize\n",
      " crap\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 321:  dangerous\n",
      " politics\n",
      " clearly\n",
      " zero\n",
      " fucker\n",
      "Cluster 322:  yes\n",
      " zero\n",
      " fucked\n",
      " forum\n",
      " forward\n",
      "Cluster 323:  gang\n",
      " little\n",
      " zero\n",
      " freedom\n",
      " fuck just\n",
      "Cluster 324:  liar\n",
      " zero\n",
      " forgot\n",
      " forum\n",
      " forward\n",
      "Cluster 325:  suck dick\n",
      " dick\n",
      " suck\n",
      " bitches\n",
      " pussy\n",
      "Cluster 326:  time\n",
      " got\n",
      " waste time\n",
      " waste\n",
      " comment\n",
      "Cluster 327:  ppl\n",
      " negative\n",
      " zero\n",
      " fucked\n",
      " fox\n",
      "Cluster 328:  willing\n",
      " share\n",
      " zero\n",
      " fuck just\n",
      " forum\n",
      "Cluster 329:  true\n",
      " end\n",
      " lol\n",
      " zero\n",
      " forum\n",
      "Cluster 330:  argument\n",
      " fans\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 331:  park\n",
      " bed\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 332:  history\n",
      " know\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 333:  thing\n",
      " make\n",
      " fuck\n",
      " friggin\n",
      " friends\n",
      "Cluster 334:  disqus\n",
      " pathetic\n",
      " cnn\n",
      " xa0you\n",
      " fuck\n",
      "Cluster 335:  troll\n",
      " bitch\n",
      " fuckin\n",
      " fox\n",
      " fox news\n",
      "Cluster 336:  clinton\n",
      " president\n",
      " fuckin\n",
      " fox\n",
      " fox news\n",
      "Cluster 337:  just stupid\n",
      " stupid\n",
      " just\n",
      " liberal\n",
      " lol\n",
      "Cluster 338:  mouth\n",
      " talk\n",
      " fucked\n",
      " forum\n",
      " forward\n",
      "Cluster 339:  wrong\n",
      " idiot\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 340:  need\n",
      " fuck\n",
      " learn\n",
      " xa0\n",
      " man\n",
      "Cluster 341:  xa0\n",
      " xa0 xa0\n",
      " said\n",
      " ex\n",
      " swallow\n",
      "Cluster 342:  boys\n",
      " sure\n",
      " zero\n",
      " fucker\n",
      " fox news\n",
      "Cluster 343:  hole\n",
      " black\n",
      " zero\n",
      " fucker\n",
      " fox\n",
      "Cluster 344:  kind\n",
      " inside\n",
      " xa0\n",
      " hard\n",
      " free\n",
      "Cluster 345:  50\n",
      " zero\n",
      " form\n",
      " forward\n",
      " fox\n",
      "Cluster 346:  smell\n",
      " wake\n",
      " wish\n",
      " rat\n",
      " current\n",
      "Cluster 347:  english\n",
      " language\n",
      " second\n",
      " ni\n",
      " want\n",
      "Cluster 348:  looking\n",
      " face\n",
      " just\n",
      " form\n",
      " forward\n",
      "Cluster 349:  sh\n",
      " holy\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 350:  cars\n",
      " like\n",
      " oh\n",
      " fuck\n",
      " zero\n",
      "Cluster 351:  agree\n",
      " just\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 352:  cool\n",
      " long\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 353:  totally\n",
      " agree\n",
      " said\n",
      " fuck\n",
      " free\n",
      "Cluster 354:  er\n",
      " nice\n",
      " fucking\n",
      " fox news\n",
      " fraud\n",
      "Cluster 355:  feed\n",
      " ya\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 356:  man\n",
      " xa0\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 357:  hot\n",
      " fucking\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 358:  apparently\n",
      " idiot\n",
      " zero\n",
      " fuck just\n",
      " forward\n",
      "Cluster 359:  thats\n",
      " pathetic\n",
      " fuck\n",
      " zero\n",
      " fucked\n",
      "Cluster 360:  horrible\n",
      " talking\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 361:  funny\n",
      " zero\n",
      " fucked\n",
      " forum\n",
      " forward\n",
      "Cluster 362:  jpg\n",
      " nhttp\n",
      " net\n",
      " profile\n",
      " com\n",
      "Cluster 363:  complete\n",
      " moron\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 364:  bury\n",
      " november\n",
      " zero\n",
      " forum\n",
      " fox\n",
      "Cluster 365:  outside\n",
      " think\n",
      " zero\n",
      " fuck just\n",
      " fuck\n",
      "Cluster 366:  tell\n",
      " didn\n",
      " zero\n",
      " form\n",
      " forward\n",
      "Cluster 367:  justice\n",
      " general\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 368:  dude\n",
      " stop\n",
      " fuck\n",
      " fucker\n",
      " forward\n",
      "Cluster 369:  didn\n",
      " work\n",
      " original\n",
      " end\n",
      " comments\n",
      "Cluster 370:  terrible\n",
      " team\n",
      " time\n",
      " zero\n",
      " friend\n",
      "Cluster 371:  xa0\n",
      " having\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 372:  offensive\n",
      " language\n",
      " zero\n",
      " fuck just\n",
      " fuck\n",
      "Cluster 373:  coming\n",
      " watch\n",
      " zero\n",
      " fucker\n",
      " fox\n",
      "Cluster 374:  dead\n",
      " blow\n",
      " zero\n",
      " fucker\n",
      " fox\n",
      "Cluster 375:  funny\n",
      " guy\n",
      " really\n",
      " forward\n",
      " fox\n",
      "Cluster 376:  moron\n",
      " fact\n",
      " zero\n",
      " fucker\n",
      " fox\n",
      "Cluster 377:  today\n",
      " yesterday\n",
      " nigga\n",
      " tomorrow\n",
      " going\n",
      "Cluster 378:  deserve\n",
      " happy\n",
      " really\n",
      " freedom\n",
      " fuck just\n",
      "Cluster 379:  waiting\n",
      " ll\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 380:  hold\n",
      " thats\n",
      " man\n",
      " gang\n",
      " fucked\n",
      "Cluster 381:  sir\n",
      " idiot\n",
      " zero\n",
      " fuck\n",
      " forum\n",
      "Cluster 382:  letting\n",
      " walk\n",
      " zero\n",
      " form\n",
      " forward\n",
      "Cluster 383:  naive\n",
      " basic\n",
      " don\n",
      " zero\n",
      " friends\n",
      "Cluster 384:  sick\n",
      " zero\n",
      " fuck just\n",
      " forum\n",
      " forward\n",
      "Cluster 385:  worst\n",
      " need\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 386:  suck\n",
      " wait\n",
      " job\n",
      " day\n",
      " zero\n",
      "Cluster 387:  pussy\n",
      " friends\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 388:  eat\n",
      " shit\n",
      " die\n",
      " live\n",
      " friggin\n",
      "Cluster 389:  bully\n",
      " weak\n",
      " zero\n",
      " fucker\n",
      " fox\n",
      "Cluster 390:  haters\n",
      " fans\n",
      " just\n",
      " fuck just\n",
      " forum\n",
      "Cluster 391:  mental\n",
      " disease\n",
      " truly\n",
      " forum\n",
      " forward\n",
      "Cluster 392:  im\n",
      " trying\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 393:  dirty\n",
      " fucker\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 394:  course\n",
      " zero\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 395:  worthless\n",
      " human\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 396:  cancer\n",
      " nuclear\n",
      " trying\n",
      " don\n",
      " zero\n",
      "Cluster 397:  ain\n",
      " dog\n",
      " forum\n",
      " forward\n",
      " fox\n",
      "Cluster 398:  number\n",
      " lol\n",
      " zero\n",
      " form\n",
      " forward\n",
      "Cluster 399:  posted\n",
      " lies\n",
      " zero\n",
      " form\n",
      " forward\n",
      "Cluster 400:  planet\n",
      " weapons\n",
      " really\n",
      " filthy\n",
      " final\n",
      "Cluster 401:  word\n",
      " love\n",
      " ll\n",
      " zero\n",
      " fucked\n",
      "Cluster 402:  david\n",
      " yes\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 403:  moved\n",
      " lol\n",
      " right\n",
      " zero\n",
      " friend\n",
      "Cluster 404:  working\n",
      " right\n",
      " zero\n",
      " fuck just\n",
      " forum\n",
      "Cluster 405:  fag\n",
      " dude\n",
      " change\n",
      " zero\n",
      " fucked\n",
      "Cluster 406:  class\n",
      " hole\n",
      " zero\n",
      " fuckin\n",
      " fox news\n",
      "Cluster 407:  jerk\n",
      " dad\n",
      " zero\n",
      " fuck just\n",
      " forum\n",
      "Cluster 408:  yep\n",
      " baby\n",
      " know\n",
      " free\n",
      " fuck\n",
      "Cluster 409:  character\n",
      " david\n",
      " 13\n",
      " main\n",
      " kind\n",
      "Cluster 410:  punk\n",
      " head\n",
      " want\n",
      " zero\n",
      " fuck\n",
      "Cluster 411:  steve\n",
      " don\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 412:  hell\n",
      " talking\n",
      " coming\n",
      " big\n",
      " forum\n",
      "Cluster 413:  called\n",
      " think\n",
      " just\n",
      " zero\n",
      " friend\n",
      "Cluster 414:  thought\n",
      " hard\n",
      " fucked\n",
      " forum\n",
      " forward\n",
      "Cluster 415:  cause\n",
      " eyes\n",
      " shoes\n",
      " really\n",
      " blind\n",
      "Cluster 416:  wont\n",
      " work\n",
      " zero\n",
      " fuck just\n",
      " forward\n",
      "Cluster 417:  lol\n",
      " zero\n",
      " form\n",
      " forward\n",
      " fox\n",
      "Cluster 418:  great\n",
      " try\n",
      " love\n",
      " zero\n",
      " forum\n",
      "Cluster 419:  freedom\n",
      " feel\n",
      " does\n",
      " zero\n",
      " fucked\n",
      "Cluster 420:  stuff\n",
      " moron\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 421:  clown\n",
      " special\n",
      " zero\n",
      " fucker\n",
      " fox\n",
      "Cluster 422:  modern\n",
      " watch\n",
      " love\n",
      " free\n",
      " fuck\n",
      "Cluster 423:  hmm\n",
      " progressive\n",
      " xa0\n",
      " free\n",
      " fuck\n",
      "Cluster 424:  dude\n",
      " moron\n",
      " zero\n",
      " friend\n",
      " fucked\n",
      "Cluster 425:  money\n",
      " ll\n",
      " won\n",
      " think\n",
      " zero\n",
      "Cluster 426:  proud\n",
      " trolls\n",
      " american\n",
      " idiot\n",
      " games\n",
      "Cluster 427:  christ\n",
      " died\n",
      " zero\n",
      " fucked\n",
      " fox\n",
      "Cluster 428:  cow\n",
      " fear\n",
      " die\n",
      " shit\n",
      " zero\n",
      "Cluster 429:  killed\n",
      " fucking\n",
      " zero\n",
      " fuck just\n",
      " forum\n",
      "Cluster 430:  baseball\n",
      " fan\n",
      " zero\n",
      " form\n",
      " forward\n",
      "Cluster 431:  clean\n",
      " obviously\n",
      " zero\n",
      " friend\n",
      " fucked\n",
      "Cluster 432:  pick\n",
      " oh\n",
      " don\n",
      " fucker\n",
      " fuck just\n",
      "Cluster 433:  face\n",
      " getting\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 434:  country\n",
      " xa0\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 435:  missing\n",
      " thought\n",
      " fucked\n",
      " forum\n",
      " forward\n",
      "Cluster 436:  imagine\n",
      " just\n",
      " like\n",
      " zero\n",
      " fuck\n",
      "Cluster 437:  brother\n",
      " post\n",
      " just\n",
      " zero\n",
      " forward\n",
      "Cluster 438:  did\n",
      " zero\n",
      " forward\n",
      " fox\n",
      " fox news\n",
      "Cluster 439:  basement\n",
      " mom\n",
      " doing\n",
      " freedom\n",
      " fuck just\n",
      "Cluster 440:  good\n",
      " stake\n",
      " mom\n",
      " night\n",
      " ok\n",
      "Cluster 441:  correct\n",
      " israel\n",
      " totally\n",
      " did\n",
      " freedom\n",
      "Cluster 442:  thanks\n",
      " man\n",
      " fucked\n",
      " forum\n",
      " forward\n",
      "Cluster 443:  bite\n",
      " door\n",
      " fuckin\n",
      " fox\n",
      " fox news\n",
      "Cluster 444:  usual\n",
      " sorry\n",
      " wrong\n",
      " fuck just\n",
      " forum\n",
      "Cluster 445:  clown\n",
      " fuckin\n",
      " bitch\n",
      " friend\n",
      " fucked\n",
      "Cluster 446:  agenda\n",
      " progressive\n",
      " vote\n",
      " zero\n",
      " free\n",
      "Cluster 447:  troll\n",
      " hard\n",
      " fucked\n",
      " forum\n",
      " forward\n",
      "Cluster 448:  nyour\n",
      " idiot\n",
      " zero\n",
      " fucked\n",
      " fox\n",
      "Cluster 449:  te\n",
      " hahaha\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 450:  tell\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 451:  look like\n",
      " look\n",
      " like\n",
      " whats\n",
      " morons\n",
      "Cluster 452:  wanted\n",
      " non\n",
      " zero\n",
      " form\n",
      " forward\n",
      "Cluster 453:  try\n",
      " little\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 454:  cast\n",
      " vote\n",
      " zero\n",
      " fucker\n",
      " fox\n",
      "Cluster 455:  break\n",
      " ll\n",
      " fuck\n",
      " like\n",
      " freedom\n",
      "Cluster 456:  supporting\n",
      " yeah\n",
      " zero\n",
      " freedom\n",
      " fuck\n",
      "Cluster 457:  world\n",
      " best\n",
      " zero\n",
      " fox\n",
      " fox news\n",
      "Cluster 458:  head\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 459:  ha\n",
      " ha ha\n",
      " dumb\n",
      " order\n",
      " basketball\n",
      "Cluster 460:  cancer\n",
      " thought\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 461:  ck\n",
      " sir\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 462:  players\n",
      " won\n",
      " zero\n",
      " fuck just\n",
      " forum\n",
      "Cluster 463:  trillion\n",
      " stupidity\n",
      " fucked\n",
      " forward\n",
      " fox\n",
      "Cluster 464:  lets\n",
      " shut\n",
      " calling\n",
      " moron\n",
      " start\n",
      "Cluster 465:  fag\n",
      " fuck\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 466:  dirt\n",
      " poor\n",
      " zero\n",
      " fucked\n",
      " fox\n",
      "Cluster 467:  man\n",
      " horrible\n",
      " knows\n",
      " friend\n",
      " fucked\n",
      "Cluster 468:  winning\n",
      " dude\n",
      " fucked\n",
      " forum\n",
      " forward\n",
      "Cluster 469:  married\n",
      " turd\n",
      " zero\n",
      " fucker\n",
      " fox\n",
      "Cluster 470:  yeah\n",
      " fuck\n",
      " killing\n",
      " ass\n",
      " freedom\n",
      "Cluster 471:  names\n",
      " aren\n",
      " zero\n",
      " fucked\n",
      " fox\n",
      "Cluster 472:  mike\n",
      " say\n",
      " want\n",
      " zero\n",
      " friggin\n",
      "Cluster 473:  close\n",
      " mouth\n",
      " fuckin\n",
      " fox\n",
      " fox news\n",
      "Cluster 474:  popularity\n",
      " said\n",
      " zero\n",
      " fuck just\n",
      " forum\n",
      "Cluster 475:  weeks\n",
      " gets\n",
      " like\n",
      " zero\n",
      " freedom\n",
      "Cluster 476:  good\n",
      " zero\n",
      " fuck just\n",
      " forum\n",
      " forward\n",
      "Cluster 477:  heard\n",
      " sure\n",
      " life\n",
      " zero\n",
      " forward\n",
      "Cluster 478:  crap\n",
      " said\n",
      " zero\n",
      " friend\n",
      " fuck just\n",
      "Cluster 479:  modern\n",
      " victim\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 480:  fake\n",
      " fucked\n",
      " forum\n",
      " forward\n",
      " fox\n",
      "Cluster 481:  come\n",
      " man\n",
      " garbage\n",
      " fucked\n",
      " forward\n",
      "Cluster 482:  typical\n",
      " troll\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 483:  17\n",
      " ll\n",
      " fuck\n",
      " zero\n",
      " friend\n",
      "Cluster 484:  word\n",
      " suck\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 485:  broken\n",
      " okay\n",
      " fucker\n",
      " forward\n",
      " fox\n",
      "Cluster 486:  il\n",
      " le\n",
      " zero\n",
      " fuck just\n",
      " forward\n",
      "Cluster 487:  usa\n",
      " muslim\n",
      " compared\n",
      " clean\n",
      " looks\n",
      "Cluster 488:  proof\n",
      " positive\n",
      " posts\n",
      " read\n",
      " zero\n",
      "Cluster 489:  help\n",
      " need\n",
      " zero\n",
      " fucked\n",
      " fox\n",
      "Cluster 490:  em\n",
      " fuck\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 491:  value\n",
      " posts\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 492:  interview\n",
      " ndid\n",
      " did\n",
      " fox\n",
      " fox news\n",
      "Cluster 493:  doing\n",
      " fuck\n",
      " zero\n",
      " fucked\n",
      " forward\n",
      "Cluster 494:  anger\n",
      " speech\n",
      " freedom\n",
      " issues\n",
      " zero\n",
      "Cluster 495:  watching\n",
      " nwhat\n",
      " game\n",
      " zero\n",
      " freedom\n",
      "Cluster 496:  weren\n",
      " right\n",
      " fuck\n",
      " zero\n",
      " freedom\n",
      "Cluster 497:  mean\n",
      " didn\n",
      " make\n",
      " fucked\n",
      " fuck just\n",
      "Cluster 498:  clueless\n",
      " completely\n",
      " zero\n",
      " fucker\n",
      " fox\n",
      "Cluster 499:  dick\n",
      " real\n",
      " zero\n",
      " fucker\n",
      " fox\n"
     ]
    }
   ],
   "source": [
    "order_centroids = k_means.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = tfidf_vect.get_feature_names()\n",
    "for i in range(num_clusters):\n",
    "    print \"Cluster %d:\" % i,\n",
    "    for ind in order_centroids[i, :5]:\n",
    "        print ' %s' % terms[ind],\n",
    "        print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of different metrics exist that allow us to measure how well the clusters fit the known distribution of underlying newsgroups. One such metric is the homogeneity which is a measure of how pure the clusters are with respect to the known labels (e.g. insult or neutral)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.196\n"
     ]
    }
   ],
   "source": [
    "print \"Homogeneity: %0.3f\" % metrics.homogeneity_score(y_train, k_means.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homogeneity scores vary between 0 and 1; a score of 1 indicates that the clusters match the original label distribution exactly.\n",
    "\n",
    "Explore what happens if you make the number of clusters larger. What do you notice? Do the clusters of posts to the mailing lists begin to make more intuitive sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have enjoyed this tutorial, the exercises in [Working With Data](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) are a good next step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
